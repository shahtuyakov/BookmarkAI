services:
  # LLM Summarization Worker
  llm-worker:
    build:
      context: ../python
      dockerfile: llm-service/Dockerfile
    container_name: bookmarkai-llm-worker
    ports:
      - '9091:9091'  # Prometheus metrics endpoint
    environment:
      # Celery configuration
      CELERY_BROKER_URL: amqp://ml:ml_password@rabbitmq:5672/
      CELERY_RESULT_BACKEND: redis://redis:6379/1
      
      # Database configuration
      DATABASE_URL: postgresql://bookmarkai:bookmarkai_password@postgres:5432/bookmarkai_dev
      
      # Redis for singleton locks
      REDIS_URL: redis://redis:6379/0
      
      # Worker configuration
      WORKER_CONCURRENCY: 4
      WORKER_PREFETCH_MULTIPLIER: 8
      WORKER_MAX_TASKS_PER_CHILD: 50
      
      # LLM configuration (will be populated from environment)
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      
      # Cost controls
      LLM_DAILY_COST_LIMIT: ${LLM_DAILY_COST_LIMIT:-20.00}
      LLM_HOURLY_COST_LIMIT: ${LLM_HOURLY_COST_LIMIT:-2.00}
      LLM_BUDGET_STRICT_MODE: ${LLM_BUDGET_STRICT_MODE:-false}
      
      # Prometheus metrics
      WORKER_TYPE: llm
      SERVICE_NAME: llm-service
      PROMETHEUS_METRICS_PORT: 9091
      PROMETHEUS_MULTIPROC_DIR: /tmp/prometheus_multiproc_llm
      
      # Database configuration for cost tracking
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: bookmarkai_dev
      POSTGRES_USER: bookmarkai
      POSTGRES_PASSWORD: bookmarkai_password
      
      # Logging
      LOG_LEVEL: INFO
    # RabbitMQ is now in main docker-compose.yml
    external_links:
      - ml-rabbitmq:rabbitmq
    volumes:
      - ../python/llm-service:/app
      - ../python/shared:/app/shared:ro
    command: >
      celery -A llm_service.celery_app worker
      --loglevel=info
      --concurrency=4
      --queues=ml.summarize
      --without-gossip
      --without-heartbeat
      --prefetch-multiplier=8
      --max-tasks-per-child=50
    networks:
      - bookmarkai-ml
      - bookmarkai-main  # To access postgres and redis
    restart: unless-stopped

  # Whisper Transcription Worker
  whisper-worker:
    build:
      context: ../python
      dockerfile: whisper-service/Dockerfile
    container_name: bookmarkai-whisper-worker
    ports:
      - '9092:9092'  # Prometheus metrics endpoint
    environment:
      # Celery configuration
      CELERY_BROKER_URL: amqp://ml:ml_password@rabbitmq:5672/
      CELERY_RESULT_BACKEND: redis://redis:6379/1
      
      # Database configuration
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: bookmarkai_dev
      POSTGRES_USER: bookmarkai
      POSTGRES_PASSWORD: bookmarkai_password
      
      # Redis for singleton locks
      REDIS_URL: redis://redis:6379/0
      
      # Worker configuration
      WORKER_CONCURRENCY: 4
      WORKER_PREFETCH_MULTIPLIER: 8
      WORKER_MAX_TASKS_PER_CHILD: 50
      
      # OpenAI configuration
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      
      # Cost controls
      WHISPER_DAILY_COST_LIMIT: ${WHISPER_DAILY_COST_LIMIT:-10.00}
      WHISPER_HOURLY_COST_LIMIT: ${WHISPER_HOURLY_COST_LIMIT:-1.00}
      
      # Silence detection
      WHISPER_SILENCE_THRESHOLD_DB: ${WHISPER_SILENCE_THRESHOLD_DB:--40.0}
      
      # Prometheus metrics
      WORKER_TYPE: whisper
      SERVICE_NAME: whisper-service
      PROMETHEUS_METRICS_PORT: 9092
      PROMETHEUS_MULTIPROC_DIR: /tmp/prometheus_multiproc_whisper
      
      # Logging
      LOG_LEVEL: INFO
      PYTHONUNBUFFERED: 1
    external_links:
      - ml-rabbitmq:rabbitmq
    volumes:
      - ../python/whisper-service:/app/whisper-service
      - ../python/shared:/app/shared:ro
      - /tmp/bookmarkai-videos:/tmp/bookmarkai-videos  # Shared video storage with local API gateway
    command: >
      celery -A whisper_service.celery_app worker
      --loglevel=info
      --concurrency=4
      --queues=ml.transcribe
      --without-gossip
      --without-heartbeat
      --prefetch-multiplier=8
      --max-tasks-per-child=50
    networks:
      - bookmarkai-ml
      - bookmarkai-main  # To access postgres and redis
    restart: unless-stopped

  # Vector Embedding Worker
  vector-worker:
    build:
      context: ../python
      dockerfile: vector-service/Dockerfile
    container_name: bookmarkai-vector-worker
    ports:
      - '9093:9093'  # Prometheus metrics endpoint
    environment:
      # Celery configuration
      CELERY_BROKER_URL: amqp://ml:ml_password@rabbitmq:5672/
      CELERY_RESULT_BACKEND: redis://redis:6379/1
      
      # Database configuration
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: bookmarkai_dev
      POSTGRES_USER: bookmarkai
      POSTGRES_PASSWORD: bookmarkai_password
      
      # Redis for singleton locks
      REDIS_URL: redis://redis:6379/0
      
      # Worker configuration
      WORKER_CONCURRENCY: 4
      WORKER_PREFETCH_MULTIPLIER: 8
      WORKER_MAX_TASKS_PER_CHILD: 50
      
      # OpenAI configuration
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      
      # Model selection thresholds
      VECTOR_SMALL_THRESHOLD: ${VECTOR_SMALL_THRESHOLD:-1000}
      VECTOR_LARGE_THRESHOLD: ${VECTOR_LARGE_THRESHOLD:-5000}
      VECTOR_DEFAULT_MODEL: ${VECTOR_DEFAULT_MODEL:-text-embedding-3-small}
      
      # Batch processing
      VECTOR_BATCH_SIZE: ${VECTOR_BATCH_SIZE:-100}
      
      # Cost controls
      VECTOR_DAILY_COST_LIMIT: ${VECTOR_DAILY_COST_LIMIT:-10.00}
      VECTOR_HOURLY_COST_LIMIT: ${VECTOR_HOURLY_COST_LIMIT:-1.00}
      VECTOR_BUDGET_STRICT_MODE: ${VECTOR_BUDGET_STRICT_MODE:-false}
      
      # Prometheus metrics
      WORKER_TYPE: vector
      SERVICE_NAME: vector-service
      PROMETHEUS_METRICS_PORT: 9093
      PROMETHEUS_MULTIPROC_DIR: /tmp/prometheus_multiproc_vector
      
      # Logging
      LOG_LEVEL: INFO
      PYTHONUNBUFFERED: 1
    external_links:
      - ml-rabbitmq:rabbitmq
    volumes:
      - ../python/vector-service:/app
      - ../python/shared:/app/shared:ro
      - /tmp/bookmarkai-videos:/tmp/bookmarkai-videos  # Bind mount to local filesystem (for future use)
    command: >
      celery -A vector_service.celery_app worker
      --loglevel=info
      --concurrency=4
      --queues=ml.embed
      --without-gossip
      --without-heartbeat
      --prefetch-multiplier=8
      --max-tasks-per-child=50
    networks:
      - bookmarkai-ml
      - bookmarkai-main  # To access postgres and redis
    restart: unless-stopped

  # Flower for Celery monitoring (optional, for development)
  flower:
    image: mher/flower:2.0
    container_name: bookmarkai-flower
    environment:
      CELERY_BROKER_URL: amqp://ml:ml_password@rabbitmq:5672/
      FLOWER_PORT: 5555
      FLOWER_BASIC_AUTH: admin:bookmarkai123  # Change in production
    ports:
      - '5555:5555'
    external_links:
      - ml-rabbitmq:rabbitmq
    networks:
      - bookmarkai-ml
    profiles:
      - monitoring

# No named volumes needed - using bind mounts to local filesystem

networks:
  bookmarkai-ml:
    driver: bridge
  bookmarkai-main:
    external: true
    name: docker_default  # This connects to the main docker-compose network